# backend/main.py
from fastapi import FastAPI
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from openai import OpenAI
from dotenv import load_dotenv
import os
import numpy as np
import glob
from pptx import Presentation
from pypdf import PdfReader
from docx import Document
from threading import Thread

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://chatbot-frontend-plum-five.vercel.app"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# OpenAI í´ë¼ì´ì–¸íŠ¸
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# -------------------------------
# ğŸ“‚ 1. ë¬¸ì„œë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
# -------------------------------
def load_pdf_text(file_path):
    reader = PdfReader(file_path)
    text = ""
    for page in reader.pages:
        if page.extract_text():
            text += page.extract_text() + "\n"
    return text

def load_ppt_text(file_path):
    prs = Presentation(file_path)
    text = ""
    for slide in prs.slides:
        for shape in slide.shapes:
            if hasattr(shape, "text"):
                text += shape.text + "\n"
    return text

def load_docx_text(file_path):
    doc = Document(file_path)
    return "\n".join([p.text for p in doc.paragraphs])

# -------------------------------
# âœ‚ï¸ 2. í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
# -------------------------------
def split_text(text, chunk_size=300):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

# -------------------------------
# ğŸ” 3. ì„ë² ë”© í•¨ìˆ˜
# -------------------------------
def get_embedding(text):
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

# -------------------------------
# ğŸ—„ï¸ 4. ë¬¸ì„œ ë¡œë“œ & ë²¡í„°í™”
# -------------------------------
def load_all_documents(folder_path="documents"):
    chunks = []

    # PDF
    for file in glob.glob(os.path.join(folder_path, "*.pdf")):
        print(f"ğŸ“„ Loading PDF: {file}")
        chunks.extend(split_text(load_pdf_text(file)))

    # PPTX
    for file in glob.glob(os.path.join(folder_path, "*.pptx")):
        print(f"ğŸ“Š Loading PPTX: {file}")
        chunks.extend(split_text(load_ppt_text(file)))

    # DOCX
    for file in glob.glob(os.path.join(folder_path, "*.docx")):
        print(f"ğŸ“ Loading DOCX: {file}")
        chunks.extend(split_text(load_docx_text(file)))

    return chunks

# -------------------------------
# ì„œë²„ ì´ˆê¸°í™” í”Œë˜ê·¸ & ì „ì—­ ë³€ìˆ˜
# -------------------------------
chunks = []
embeddings = np.array([])
is_ready = False

def initialize_embeddings():
    global chunks, embeddings, is_ready
    print("ğŸ“‚ ë¬¸ì„œ ë¡œë”© ë° ë²¡í„°í™” ì‹œì‘...")
    chunks = load_all_documents("documents")
    embeddings = np.array([get_embedding(c) for c in chunks])
    is_ready = True
    print(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ! ì´ {len(chunks)}ê°œ ì²­í¬ ë²¡í„°í™”")

# ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ì´ˆê¸°í™”
Thread(target=initialize_embeddings).start()

# -------------------------------
# ğŸ” 5. ê²€ìƒ‰ í•¨ìˆ˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
# -------------------------------
def search(query, top_k=3):
    query_emb = np.array(get_embedding(query))
    similarities = np.dot(embeddings, query_emb) / (
        np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_emb)
    )
    top_indices = np.argsort(similarities)[::-1][:top_k]
    return [chunks[i] for i in top_indices]

# -------------------------------
# ğŸ¤– 6. ì±—ë´‡ API
# -------------------------------
@app.post("/chat")
async def chat(user_input: dict):
    global is_ready
    if not is_ready:
        return {"reply": "âš ï¸ ì„œë²„ê°€ ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", "context": []}

    query = user_input.get("message", "")
    if not query.strip():
        return {"reply": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "context": []}

    # ë¬¸ì„œ ê²€ìƒ‰
    context = search(query)

    # GPTì—ê²Œ ë¬¸ë§¥ê³¼ í•¨ê»˜ ì „ë‹¬
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ë‹µí•˜ëŠ” AIì•¼."},
            {"role": "user", "content": f"ë¬¸ì„œ ë‚´ìš©: {context}\n\nì§ˆë¬¸: {query}"}
        ]
    )

    answer = response.choices[0].message.content
    return {"reply": answer, "context": context}

# -------------------------------
# ë£¨íŠ¸ í…ŒìŠ¤íŠ¸
# -------------------------------
@app.get("/")
def root():
    status_msg = "âœ… ì„œë²„ ì‹¤í–‰ ì¤‘"
    if not is_ready:
        status_msg = "âš ï¸ ë¬¸ì„œ ë²¡í„°í™” ì§„í–‰ ì¤‘"
    return {"message": f"FastAPI RAG ì„œë²„ ì •ìƒ ì‹¤í–‰ ğŸš€ - {status_msg}"}
