# backend/main.py
from fastapi import FastAPI
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from openai import OpenAI
from dotenv import load_dotenv
import os
import numpy as np
import glob
from pptx import Presentation
from pypdf import PdfReader
from docx import Document

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://chatbot-backend-stbs.onrender.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# OpenAI í´ë¼ì´ì–¸íŠ¸
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# -------------------------------
# ğŸ“‚ 1. ë¬¸ì„œë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
# -------------------------------
def load_pdf_text(file_path):
    reader = PdfReader(file_path)
    text = ""
    for page in reader.pages:
        if page.extract_text():
            text += page.extract_text() + "\n"
    return text

def load_ppt_text(file_path):
    prs = Presentation(file_path)
    text = ""
    for slide in prs.slides:
        for shape in slide.shapes:
            if hasattr(shape, "text"):
                text += shape.text + "\n"
    return text

def load_docx_text(file_path):
    doc = Document(file_path)
    return "\n".join([p.text for p in doc.paragraphs])

# -------------------------------
# âœ‚ï¸ 2. í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
# -------------------------------
def split_text(text, chunk_size=300):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

# -------------------------------
# ğŸ” 3. ì„ë² ë”© í•¨ìˆ˜
# -------------------------------
def get_embedding(text):
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

# -------------------------------
# ğŸ—„ï¸ 4. ë¬¸ì„œ í´ë”ì—ì„œ ìë™ ë¡œë“œ & ë²¡í„°í™”
# -------------------------------
def load_all_documents(folder_path="documents"):
    chunks = []

    # PDF
    for file in glob.glob(os.path.join(folder_path, "*.pdf")):
        print(f"ğŸ“„ Loading PDF: {file}")
        chunks.extend(split_text(load_pdf_text(file)))

    # PPTX
    for file in glob.glob(os.path.join(folder_path, "*.pptx")):
        print(f"ğŸ“Š Loading PPTX: {file}")
        chunks.extend(split_text(load_ppt_text(file)))

    # DOCX
    for file in glob.glob(os.path.join(folder_path, "*.docx")):
        print(f"ğŸ“ Loading DOCX: {file}")
        chunks.extend(split_text(load_docx_text(file)))

    return chunks

# ë¬¸ì„œ ë¡œë“œ & ë²¡í„°í™”
print("ğŸ“‚ ë¬¸ì„œ ë¡œë”© ì‹œì‘...")
chunks = load_all_documents("documents")  # documents í´ë”
embeddings = [get_embedding(chunk) for chunk in chunks]
embeddings = np.array(embeddings)
print(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ! ì´ {len(chunks)}ê°œ ì²­í¬ ë²¡í„°í™”")

# -------------------------------
# ğŸ” 5. ê²€ìƒ‰ í•¨ìˆ˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
# -------------------------------
def search(query, top_k=3):
    query_emb = np.array(get_embedding(query))
    similarities = np.dot(embeddings, query_emb) / (
        np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_emb)
    )
    top_indices = np.argsort(similarities)[::-1][:top_k]
    return [chunks[i] for i in top_indices]

# -------------------------------
# ğŸ¤– 6. ì±—ë´‡ API
# -------------------------------
@app.post("/chat")
async def chat(user_input: dict):
    query = user_input.get("message", "")
    if not query.strip():
        return {"reply": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "context": []}

    # ë¬¸ì„œ ê²€ìƒ‰
    context = search(query)

    # GPTì—ê²Œ ë¬¸ë§¥ê³¼ í•¨ê»˜ ì „ë‹¬
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ë‹µí•˜ëŠ” AIì•¼."},
            {"role": "user", "content": f"ë¬¸ì„œ ë‚´ìš©: {context}\n\nì§ˆë¬¸: {query}"}
        ]
    )

    answer = response.choices[0].message.content
    return {"reply": answer, "context": context}  # GPT ë‹µë³€ + ì°¸ê³  ë¬¸ì„œ ì²­í¬

# ë£¨íŠ¸ í…ŒìŠ¤íŠ¸
@app.get("/")
def root():
    return {"message": "FastAPI RAG ì„œë²„ ì •ìƒ ì‹¤í–‰ ğŸš€"}
